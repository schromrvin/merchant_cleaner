# ğŸš€ The Hybrid Merchant Extraction Engine

A high-performance, self-contained merchant name extraction engine designed to run in a highly restricted enterprise environment. Its function is to accurately extract clean, official merchant names from raw, noisy, and often malformed transaction strings.

This engine was developed under strict constraints, including no access to cloud-based LLMs or common NLP libraries. The final, successful architecture is a **Direct Candidate Scoring Model** that intelligently filters noise and scores potential matches from a pre-built index of over 1.6 million official entity names.

## âœ¨ Key Features

*   **ğŸ¯ High Accuracy:** Correctly identifies merchants when the core name is present, even amidst significant noise (e.g., `BREADTALK CAFE-WESTGATE SINGAPORE SG` â†’ `BREADTALK PTE LTD`).
*   **âš™ï¸ Robust & Self-Contained:** Runs 100% offline with no external API calls. The entire logic is built from the ground up using standard Python data science libraries.
*   **âš¡ Performant:** Utilizes a pre-built inverted index (`match_index.pkl`) for near-instantaneous retrieval of relevant merchant candidates from a database of millions.
*   **ğŸ› ï¸ Intelligent Scoring:** Employs a weighted Dice Coefficient to score matches, giving more importance to rare, distinctive words (`"breadtalk"`, `"liho"`) over common ones (`"cafe"`, `"bakery"`).
*   **âŒ¨ï¸ Interactive CLI:** Includes a simple and effective command-line interface for rapid testing and validation.

## ğŸ›ï¸ Architecture Overview

The project's success was the result of significant iteration. Initial attempts involving more complex pipelines (like a CRF model followed by an XGBoost cleaner) proved too brittle and were ultimately deprecated.

The final, definitive architecture is a **Direct Candidate Scoring Model**, which works as follows:

1.  **Universal Noise Filtering:** The system uses a comprehensive, manually curated list of over 200 "stop words" (e.g., `mall`, `singapore`, `txn`, `pte`, `ltd`).
2.  **Core Token Extraction:** A raw transaction string is tokenized, and all stop words are aggressively removed, leaving only the core, meaningful tokens (e.g., `"SUBWAY - AMK HUB SINGAPORE SG"` â†’ `{'subway'}`).
3.  **Candidate Retrieval:** These core tokens are used to retrieve a small, relevant list of potential official merchant names from a pre-built inverted index.
4.  **Weighted Scoring:** Each candidate is also filtered using the same stop words list. A **Weighted Dice Coefficient** is then used to score the similarity between the core input tokens and the core candidate tokens. This score reflects how completely the official merchant name is represented in the transaction string.

This non-destructive, direct-scoring approach proved to be far more robust and accurate than the earlier multi-stage pipelines.

## âš ï¸ Project Context & Development Constraints

This project was developed during a polytechnic internship within a major financial institution. The design and implementation were heavily shaped by the strict operational environment.

*   **ğŸ”’ Strict Security & Privacy:** Due to the sensitive nature of financial data, the solution had to be **100% on-premise and self-contained**. No cloud services or external API calls (e.g., Google NLP, OpenAI) were permitted.
*   **ğŸŒ Air-Gapped Environment:** Production-like systems had no internet access, meaning all dependencies had to be vetted and managed in a controlled manner.
*   **ğŸš« Forbidden Packages:** To minimize security vulnerabilities and simplify auditing, the use of large, complex NLP libraries was strictly forbidden. This included:
    *   `spaCy`
    *   `NLTK`
    *   `fastText`
    *   `BERT` or any `Hugging Face Transformers`
*   **ğŸ¦ Legacy Systems:** The solution needed to be compatible with a legacy analytics environment, favoring robust, well-established data science packages (`pandas`, `xgboost`, `scikit-learn` ecosystem) over more modern or experimental ones.

## ğŸ“‚ Folder Structure

The project is organized into a modular structure separating data, models, training scripts, and the final application logic.

```
merchant_cleaner/
â”‚
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ acra_entities.csv             # Source data file (can be updated)
â”‚   â”œâ”€â”€ other_uen_entities.csv        # Source data file (can be updated)
â”‚   â””â”€â”€ synthetic_training_data.csv   # Synthetic training dataset generated by prepare_acra_data.py
â”‚
â”œâ”€â”€ ğŸ“‚ model/
â”‚   â”œâ”€â”€ specialist_crf_model.pkl      # The trained CRF model
â”‚   â”œâ”€â”€ xgboost_cleaner_model.pkl     # The trained XGBoost model
â”‚   â”œâ”€â”€ token_frequencies.pkl         # Frequency map of all tokens
â”‚   â””â”€â”€ match_index.pkl               # The inverted index for fast lookups
â”‚
â”œâ”€â”€ ğŸ“‚ training/
â”‚   â”œâ”€â”€ prepare_acra_data.py          # Script to generate synthetic training data
â”‚   â”œâ”€â”€ train_specialist_model.py     # Script to train the CRF model
â”‚   â”œâ”€â”€ train_xgboost_cleaner.py      # Script to train the XGBoost cleaner
â”‚   â”œâ”€â”€ create_token_frequencies.py   # Creates the token frequency map
â”‚   â””â”€â”€ build_match_index.py          # Creates the matching index
â”‚
â”œâ”€â”€ ğŸ“‚ app/
â”‚   â”œâ”€â”€ inference.py                  # Direct-Scoring inference logic
â”‚   â””â”€â”€ cli_shell.py                  # Interactive command-line interface
â”‚
â”œâ”€â”€ ğŸ“‚ processing/
â”‚   â”œâ”€â”€ tokenizer.py                  # Robust tokenizer utility
â”‚   â”œâ”€â”€ feature_extractor.py          # Feature utility for the legacy CRF model
â”‚   â”œâ”€â”€ xgboost_feature_extractor.py  # Feature utility for the legacy XGBoost model
â”‚   â””â”€â”€ label_generator.py            # Label utility for the legacy CRF model
â”‚
â”œâ”€â”€ .gitignore                        # Git ignore file
â”œâ”€â”€ README.md                         # This project documentation file
â””â”€â”€ requirements.txt                  # Python package requirements
```

## ğŸ“Š Dataset Sources

The core of this project is a comprehensive list of all officially registered business entities in Singapore.

*   **Included Data:** This repository includes versions of the datasets in the `data/` folder to ensure the project is runnable out-of-the-box.
*   **Recommended Update:** For the highest accuracy and most up-to-date merchant names, it is **highly recommended** to download the latest versions of these files from Singapore's official data portal and replace the files in the `data/` directory.
    *   **ACRA Registered Entities:** [data.gov.sg](https://data.gov.sg/datasets/d_3f960c10fed6145404ca7b821f263b87/view)
    *   **Other UEN Entities:** [data.gov.sg](https://data.gov.sg/datasets/d_b1d2b840ab9e993570c037b706b39bb8/view)

## ğŸ“¦ Setup and Installation

**1. Clone the Repository**

```bash
git clone https://github.com/schromrvin/merchant_cleaner
cd merchant_cleaner
```

**2. Create and Activate a Virtual Environment**

```bash
# For Mac/Linux
python3 -m venv venv
source venv/bin/activate

# For Windows
python -m venv venv
.\venv\Scripts\activate
```

**3. Install Required Packages**

```bash
pip install -r requirements.txt
```

## ğŸš€ Step-by-Step Execution

This project can be run in two ways: you can immediately use the pre-trained models included in this repository, or you can perform a full re-training pipeline from scratch using the raw data.

### **Part A: Quick Start (âœ… Recommended Method)**

This is the fastest way to get the application running. This method uses the pre-built model artifacts (`.pkl` files) located in the `model/` directory.

**1. Complete the Setup**

Ensure you have completed the steps in the **ğŸ“¦ Setup and Installation** section (cloned the repo, activated your virtual environment, and installed the requirements).

**2. Run the Application Directly**

Launch the interactive command-line interface. It will automatically load the pre-trained models and be ready for use.

```bash
python app/cli_shell.py
```

You will be prompted to enter raw transaction strings. Type `exit` or `quit` to stop the application.

**Example Usage:**

```
Initializing Processor with AGGRESSIVE FILTERING Logic...
âœ… Index and token frequencies loaded successfully.
Ready for testing. Type 'exit' or 'quit' to stop.

Enter Transaction String: BREADTALK CAFE-WESTGATE SINGAPORE SG
------------------------------------------------------------
Input Text: BREADTALK CAFE-WESTGATE SINGAPORE SG
Status: Match Found (Confidence: 65.6%)
Official Name: BREADTALK PTE LTD
------------------------------------------------------------
```

### **Part B: Full Re-Training Pipeline (ğŸ› ï¸ Advanced)**

Follow these steps only if you have updated the source data in the `data/` folder, or if you want to regenerate all model artifacts from scratch.

**Important:** The scripts must be run in the following order, as they depend on each other's outputs.

**1. Generate Synthetic Training Data**

This script creates the `synthetic_training_data.csv` file, which is required by the machine learning models.

```bash
python training/prepare_acra_data.py
```

**2. Train the CRF Specialist Model**

This trains the CRF model.

```bash
python training/train_specialist_model.py
```

**3. Train the XGBoost Cleaner Model**

This trains the XGBoost cleaner model.

```bash
python training/train_xgboost_cleaner.py
```

**4. Create the Token Frequency File**

This script reads the raw data and creates the crucial `token_frequencies.pkl` file, which is essential for the final model's weighted scoring logic.

```bash
python training/create_token_frequencies.py
```

**5. Build the Matching Index**

This script creates the final `match_index.pkl` inverted index from all official entity names, which enables fast candidate lookups.

```bash
python training/build_match_index.py
```

**6. Run the Application**

After all artifacts have been successfully regenerated, you can launch the application.

```bash
python app/cli_shell.py
```

## ğŸ“ˆ Model Performance, Limitations, and Future Work

### Current Performance

The final Direct Candidate Scoring model represents a significant improvement over previous iterations I tested. It excels at identifying merchants when the core name is present. However, its accuracy can be described as **good but not perfect**. Many correct matches are found but may fall just below the high-confidence threshold.

### Known Limitations

1.  **Typographical Errors:** The current matching logic relies on exact token matches after cleaning (e.g., `breadtalk` matches `breadtalk`). It has **no built-in tolerance for typos** or misspellings (e.g., it cannot match `bredtalk` to `breadtalk`).
2.  **Truly Unseen Merchants:** The model's knowledge is limited to the entities in the datasets. It cannot identify a brand new merchant (e.g., a foreign merchant like `"CANVAPTYLIM"`) if that name does not exist in its index.
3.  **Ambiguous Overlap:** In rare cases where a merchant name contains another, shorter merchant name (e.g., `"STARBUCKS COFFEE AT THE STAR VISTA"`), the scoring logic may occasionally prefer the shorter or more common entity if the context is noisy.

### Future Development

This project has a clear path forward for significant accuracy improvements:

1.  **ğŸ’¡ Implement Fuzzy Matching:** The most critical next step is to replace the exact token matching with a fuzzy matching algorithm.
    *   **Method:** Integrate a library like `thefuzz` or implement a `Levenshtein distance` or `Jaro-Winkler similarity` calculation.
    *   **Impact:** This would allow the model to overcome typos and minor variations in spelling, dramatically improving recall.
2.  **ğŸ§  Dynamic Confidence Threshold:** The current confidence threshold is fixed. A more advanced system could adjust this threshold dynamically.
    *   **Method:** Lower the confidence requirement if the matching tokens are extremely rare and distinctive (e.g., `scarlett`), and raise it if the match is based on more common words (`cafe`, `bakery`).
    *   **Impact:** This would improve precision by rejecting weak matches on common words while correctly identifying strong matches on unique ones.
3.  **ğŸ”¬ Re-introduce a Boundary Detection Model:** For handling truly unseen merchants, a machine learning model could be trained *only* to find the most likely start and end of a merchant name.
    *   **Method:** Re-train the CRF model with much more varied synthetic data, focusing solely on `B-I-E-S` tagging.
    *   **Impact:** When the primary scoring model fails, this boundary detection model could provide a high-quality "best guess" extraction from the raw string (e.g., extracting `"CANVAPTYLIM"` from `PAYPAL *CANVAPTYLIM...`).